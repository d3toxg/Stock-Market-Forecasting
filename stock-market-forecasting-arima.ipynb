{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Load all the required libraries","metadata":{}},{"cell_type":"code","source":"!pip install pmdarima","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-05T17:04:56.282195Z","iopub.execute_input":"2023-01-05T17:04:56.282941Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f106ee8ce10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pmdarima/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f106ee84310>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pmdarima/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f106ee84650>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pmdarima/\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 6\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom pmdarima.arima import auto_arima\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d')\nstock_data = pd.read_csv('../input/price-volume-data-for-all-us-stocks-etfs/Stocks/acgl.us.txt',sep=',', index_col='Date', parse_dates=['Date'], date_parser=dateparse).fillna(0)\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the per day closing price of the stock.","metadata":{}},{"cell_type":"code","source":"#plot close price\nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.xlabel('Date')\nplt.ylabel('Close Prices')\nplt.plot(stock_data['Close'])\nplt.title('ARCH CAPITAL GROUP closing price')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can also visualize the data in our series through a probability distribution too.","metadata":{}},{"cell_type":"code","source":"#Distribution of the dataset\ndf_close.plot(kind='kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, a given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.\n\nThese components are defined as follows:\n\n1. **Level**: The average value in the series.\n\n2. **Trend**: The increasing or decreasing value in the series.\n\n3. **Seasonality**: The repeating short-term cycle in the series.\n\n4. **Noise**: The random variation in the series.\n\nFirst, we need to check if a series is stationary or not because time series analysis only works with stationary data.\n\n\n**ADF (Augmented Dickey-Fuller) Test**\n\nThe Dickey-Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test is:\n\n\n**Null Hypothesis**: The series has a unit root (value of a =1)\n\n\n**Alternate Hypothesis**: The series has no unit root.\n\n\nIf we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary.\n\n\nIf both mean and standard deviation are flat lines(constant mean and constant variance), the series becomes stationary.\n\n\n**So let's check for stationarity:","metadata":{}},{"cell_type":"code","source":"#Test for staionarity\ndef test_stationarity(timeseries):\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(12).mean()\n    rolstd = timeseries.rolling(12).std()\n    #Plot rolling statistics:\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(rolmean, color='red', label='Rolling Mean')\n    plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show(block=False)\n    \n    print(\"Results of dickey fuller test\")\n    adft = adfuller(timeseries,autolag='AIC')\n    # output for dft will give us without defining what the values are.\n    #hence we manually write what values does it explains using a for loop\n    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n    for key,values in adft[4].items():\n        output['critical value (%s)'%key] =  values\n    print(output)\n    \ntest_stationarity(df_close)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Through the above graph, we can see the increasing mean and standard deviation and hence our series is not stationary.","metadata":{}},{"cell_type":"markdown","source":"We see that the p-value is greater than 0.05 so we cannot reject the Null hypothesis. Also, the test statistics is greater than the critical values. so the data is non-stationary.\n\n\nIn order to perform a time series analysis, we may need to separate seasonality and trend from our series. The resultant series will become stationary through this process.\n\n\nSo let us separate Trend and Seasonality from the time series.","metadata":{}},{"cell_type":"code","source":"#To separate the trend and the seasonality from a time series, \n# we can decompose the series using the following code.\nresult = seasonal_decompose(df_close, model='multiplicative', freq = 30)\nfig = plt.figure()  \nfig = result.plot()  \nfig.set_size_inches(16, 9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by taking a log of the series to reduce the magnitude of the values and reduce the rising trend in the series. Then after getting the log of the series, we find the rolling average of the series. A rolling average is calculated by taking input for the past 12 months and giving a mean consumption value at every point further ahead in series.","metadata":{}},{"cell_type":"code","source":"#if not stationary then eliminate trend\n#Eliminate trend\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 6\ndf_log = np.log(df_close)\nmoving_avg = df_log.rolling(12).mean()\nstd_dev = df_log.rolling(12).std()\nplt.legend(loc='best')\nplt.title('Moving Average')\nplt.plot(std_dev, color =\"black\", label = \"Standard Deviation\")\nplt.plot(moving_avg, color=\"red\", label = \"Mean\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to create an ARIMA model and will train it with the closing price of the stock on the train data. So let us split the data into training and test set and visualize it.","metadata":{}},{"cell_type":"code","source":"\n#split data into train and training set\ntrain_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.xlabel('Dates')\nplt.ylabel('Closing Prices')\nplt.plot(df_log, 'green', label='Train data')\nplt.plot(test_data, 'blue', label='Test data')\nplt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its time to choose parameters p,q,d for ARIMA model. Last time we chose the value of p,d, and q by observing the plots of ACF and PACF but now we are going to use Auto ARIMA to get the best parameters without even plotting ACF and PACF graphs.","metadata":{}},{"cell_type":"markdown","source":"#### Auto ARIMA: Automatically discover the optimal order for an ARIMA model.\n\n\nThe auto_arima function seeks to identify the most optimal parameters for an ARIMA model, and returns a fitted ARIMA model. This function is based on the commonly-used R function, **forecast::auto.arima**.\n\n\nThe **auro_arima** function works by conducting differencing tests (i.e., **Kwiatkowski–Phillips–Schmidt–Shin, Augmented Dickey-Fuller or Phillips–Perron**) to determine the order of differencing, d, and then fitting models within ranges of defined start_p, max_p, start_q, max_q ranges. If the seasonal optional is enabled, auto_arima also seeks to identify the optimal P and Q hyper- parameters after conducting the **Canova-Hansen** to determine the optimal order of seasonal differencing, D.","metadata":{}},{"cell_type":"code","source":"model_autoARIMA = auto_arima(train_data, start_p=0, start_q=0,\n                      test='adf',       # use adftest to find optimal 'd'\n                      max_p=3, max_q=3, # maximum p and q\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\nprint(model_autoARIMA.summary())\nmodel_autoARIMA.plot_diagnostics(figsize=(15,8))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So how to interpret the plot diagnostics?\n\n\n**Top left**: The residual errors seem to fluctuate around a mean of zero and have a uniform variance.\n\n\n**Top Right**: The density plot suggest normal distribution with mean zero.\n\n\n**Bottom left**: All the dots should fall perfectly in line with the red line. Any significant deviations would imply the distribution is skewed.\n\n\n**Bottom Right**: The Correlogram, aka, ACF plot shows the residual errors are not autocorrelated. Any autocorrelation would imply that there is some pattern in the residual errors which are not explained in the model. So you will need to look for more X’s (predictors) to the model.\n\n\nOverall, it seems to be a good fit. Let’s start forecasting the stock prices.\n\n\nNext, create an ARIMA model with provided optimal parameters p, d and q.","metadata":{}},{"cell_type":"markdown","source":"**So the Auto ARIMA model provided the value of p,d, and q as 1, 1 and 2 respectively.**","metadata":{}},{"cell_type":"code","source":"#Modeling\n# Build Model\nmodel = ARIMA(train_data, order=(1,1,2))  \nfitted = model.fit(disp=-1)  \nprint(fitted.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's start forecast the stock prices on the test dataset keeping 95% confidence level.","metadata":{}},{"cell_type":"code","source":"# Forecast\nfc, se, conf = fitted.forecast(321, alpha=0.05)  # 95% conf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the results","metadata":{}},{"cell_type":"code","source":"# Make as pandas series\nfc_series = pd.Series(fc, index=test_data.index)\nlower_series = pd.Series(conf[:, 0], index=test_data.index)\nupper_series = pd.Series(conf[:, 1], index=test_data.index)\n# Plot\nplt.figure(figsize=(10,5), dpi=100)\nplt.plot(train_data, label='training data')\nplt.plot(test_data, color = 'blue', label='Actual Stock Price')\nplt.plot(fc_series, color = 'orange',label='Predicted Stock Price')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.10)\nplt.title('ARCH CAPITAL GROUP Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('ARCH CAPITAL GROUP Stock Price')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see our model did quite handsomely. Let us also check the commonly used accuracy metrics to judge forecast results:","metadata":{}},{"cell_type":"code","source":"\n# report performance\nmse = mean_squared_error(test_data, fc)\nprint('MSE: '+str(mse))\nmae = mean_absolute_error(test_data, fc)\nprint('MAE: '+str(mae))\nrmse = math.sqrt(mean_squared_error(test_data, fc))\nprint('RMSE: '+str(rmse))\nmape = np.mean(np.abs(fc - test_data)/np.abs(test_data))\nprint('MAPE: '+str(mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Around 2.5% MAPE implies the model is about 97.5% accurate in predicting the next 15 observations.\n","metadata":{}}]}